apiVersion: v1
device: 'mps' # 'mps' for mac and 'cuda:0' for ubuntu
training_param:
  dataset_loc:
    path: ''
  checkpoints_path: './checkpoints/dialogue-summary-training-'
  learning_rate: 1e-5
  num_train_epoch: 1
  weight_decay: 0.01
  logging_steps: 1
  max_steps: 10
finetune_param:
  lora_config:
    lora_rank: 32
    lora_alpha: 32
    target_modules: ['q', 'v']
    dropout: 0.05
    bias: 'none'
    checkpoints_path: './checkpoints/peft-dialogue-summary-training-'
  peft_config:
    auto_fine_batch_size: True
    learning_rate: 1e-3
    num_train_epochs: 1
    logging_steps: 1
    max_steps: 1
